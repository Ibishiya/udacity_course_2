

# Task
Perform the CRISP-DM process on the `/content/onlinefraud.csv` dataset to predict the 'fraud' variable using a machine learning model, and calculate predictions.

## Business understanding

### Subtask:
Understand the business problem related to online fraud prediction.


**Reasoning**:
Reflect on the typical challenges and goals associated with predicting online fraud, the potential impact of successful fraud detection, potential costs of false positives and negatives, potential types of online fraud, and what a successful outcome of this project would look like from a business perspective.



# 1. Challenges and Goals:
# Challenges: Highly imbalanced datasets (fraud is rare), evolving fraud techniques, need for real-time detection,
# maintaining customer experience while detecting fraud.
# Goals: Minimize financial losses due to fraud, protect customer trust, reduce operational costs associated with manual fraud review.

# Impact:
# Businesses: Reduced financial losses, improved reputation, increased customer confidence.
# Customers: Protection from unauthorized transactions, increased trust in online platforms.

# 2. Costs of Errors:
# False Positives: Annoying legitimate customers, potential loss of business, increased operational costs for manual review.
# False Negatives: Direct financial loss due to fraud, damage to reputation, potential legal issues.

# 3. Types of Online Fraud:
# Credit card fraud, identity theft, account takeover, friendly fraud (chargebacks), synthetic identity fraud.
# Manifestation in data: Suspicious transaction patterns, unusual locations, large transaction amounts,
# multiple failed login attempts, inconsistent user behavior.

# 4. Successful Outcome:
# A successful outcome would involve building a predictive model that effectively identifies fraudulent transactions
# with a high true positive rate (recall) while maintaining a low false positive rate (precision).
# The model should be deployable in real-time and provide actionable insights for fraud prevention.
# Success metrics could include reduction in fraud losses, improved detection rate, and minimal disruption to legitimate transactions.

print("Business problem understanding completed.")

## Data understanding

### Subtask:
Load and explore the `/content/onlinefraud.csv` dataset.


**Reasoning**:
Load the dataset from `/content/onlinefraud.csv` into a pandas DataFrame named `df`, display the first 5 rows, print the concise summary, display descriptive statistics, display the count of unique values, check for and display the number of missing values, and examine the distribution of the target variable 'fraud'.



import pandas as pd

# 1. Load the dataset
df = pd.read_csv('/content/onlinefraud.csv')

# 2. Display the first 5 rows
print("First 5 rows of the DataFrame:")
display(df.head())

# 3. Print the concise summary
print("\nConcise summary of the DataFrame:")
df.info()

# 4. Display descriptive statistics of numerical columns
print("\nDescriptive statistics of numerical columns:")
display(df.describe())

# 5. Display the count of unique values for each column
print("\nNumber of unique values for each column:")
display(df.nunique())

# 6. Check for and display the number of missing values
print("\nNumber of missing values in each column:")
display(df.isnull().sum())

# 7. Examine the distribution of the target variable 'fraud'
print("\nDistribution of the target variable 'fraud':")
display(df['isFraud'].value_counts())

## Data preparation

### Subtask:
Clean and prepare the data for machine learning, including handling missing values, encoding categorical features, and feature scaling if necessary.


**Reasoning**:
Handle missing values, drop irrelevant columns, one-hot encode the 'type' column, scale numerical features, define features and target, and split the data into stratified training and testing sets.



from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import numpy as np

# 1. Handle the single missing row by dropping it
df.dropna(inplace=True)

# 2. Drop the isFlaggedFraud column as it has only one unique value and is not discriminatory.
df.drop('isFlaggedFraud', axis=1, inplace=True)

# 3. Handle categorical features: 'type' and drop 'nameOrig' and 'nameDest'
df.drop(['nameOrig', 'nameDest'], axis=1, inplace=True)
df = pd.get_dummies(df, columns=['type'], drop_first=True) # One-hot encode 'type'

# 4. Define numerical features for scaling
numerical_features = ['amount', 'oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest']

# 5. Apply standard scaler to numerical features
scaler = StandardScaler()
df[numerical_features] = scaler.fit_transform(df[numerical_features])

# 6. Define feature matrix X and target vector y
X = df.drop('isFraud', axis=1)
y = df['isFraud']

# 7. Split data into training and testing sets with stratification
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)

print("Data cleaning and preparation completed.")
print("Shape of X_train:", X_train.shape)
print("Shape of y_train:", y_train.shape)
print("Shape of X_test:", X_test.shape)
print("Shape of y_test:", y_test.shape)

## Modeling

### Subtask:
Select and train a machine learning model to predict the 'fraud' variable.


**Reasoning**:
Import and train a RandomForestClassifier model with balanced class weights to address the class imbalance.



from sklearn.ensemble import RandomForestClassifier

# Instantiate the RandomForestClassifier model with balanced class weights
model = RandomForestClassifier(class_weight='balanced', random_state=42)

# Train the model using the training data
model.fit(X_train, y_train)

print("Model training completed.")

## Evaluation

### Subtask:
Evaluate the performance of the trained model using appropriate metrics.


**Reasoning**:
Import the necessary evaluation metrics and make predictions on the test set.



from sklearn.metrics import classification_report, confusion_matrix

# Make predictions on the test set
y_pred = model.predict(X_test)

**Reasoning**:
Generate and print the classification report and confusion matrix to evaluate the model's performance.



# Generate and print the classification report
print("Classification Report:")
print(classification_report(y_test, y_pred))

# Generate and print the confusion matrix
print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred))

## Deployment

### Subtask:
Discuss how the model could be deployed.


**Reasoning**:
Discuss the deployment considerations for the trained model, covering technical infrastructure, challenges, and maintenance.



# 1. Technical Infrastructure:
print("Technical Infrastructure:")
print("- **APIs:** A RESTful API would be needed to allow other applications (e.g., e-commerce platforms, banking systems) to send transaction data for real-time fraud prediction.")
print("- **Databases:** A low-latency database would be required to store transaction data, model predictions, and potentially feature stores for fast retrieval. Examples include NoSQL databases like Cassandra or managed services like Amazon DynamoDB.")
print("- **Monitoring Systems:** Robust monitoring is crucial to track model performance (e.g., prediction latency, error rates, drift in data distribution), system health (e.g., server load, memory usage), and business metrics (e.g., fraud detection rate, false positive rate). Tools like Prometheus, Grafana, or cloud-specific monitoring services could be used.")
print("- **Deployment Platform:** The model could be deployed on cloud platforms (e.g., AWS SageMaker, Google AI Platform, Azure Machine Learning) or on-premise infrastructure using containerization (e.g., Docker) and orchestration tools (e.g., Kubernetes).")
print("- **Message Queues:** For asynchronous processing or handling bursts of traffic, message queues (e.g., Kafka, RabbitMQ) could be used to decouple the API from the prediction service.")

# 2. Challenges and Strategies for Low Latency and High Availability:
print("\nChallenges and Strategies for Low Latency and High Availability:")
print("- **Low Latency:**")
print("  - **Model Optimization:** Use efficient model architectures and optimize model inference (e.g., using tools like ONNX Runtime, TensorFlow Serving with TensorRT).")
print("  - **Infrastructure Proximity:** Deploy the model geographically close to the users or services making predictions.")
print("  - **Caching:** Cache frequently accessed data or model predictions.")
print("  - **Horizontal Scaling:** Scale the prediction service horizontally to handle increased load.")
print("- **High Availability:**")
print("  - **Redundancy:** Deploy the model and its dependencies across multiple availability zones or regions.")
print("  - **Load Balancing:** Distribute incoming requests across multiple instances of the prediction service.")
print("  - **Automated Failover:** Implement automated failover mechanisms to redirect traffic to healthy instances in case of failures.")
print("  - **Circuit Breaker Pattern:** Implement circuit breakers to prevent cascading failures when downstream services are unavailable.")

# 3. Ongoing Maintenance and Retraining:
print("\nOngoing Maintenance and Retraining:")
print("- **Monitoring Model Performance:** Continuously monitor the model's performance metrics (precision, recall, F1-score) in production to detect degradation.")
print("- **Data Drift Detection:** Monitor for changes in the distribution of incoming data compared to the training data.")
print("- **Retraining Frequency:** Establish a schedule for retraining the model with new data to capture evolving fraud patterns. This could be daily, weekly, or monthly depending on the data volume and the rate of concept drift.")
print("- **Automated Retraining Pipelines:** Implement automated pipelines for data ingestion, preprocessing, model training, validation, and deployment to streamline the retraining process.")
print("- **A/B Testing:** Use A/B testing to compare the performance of new model versions against the current production model before full rollout.")
print("- **Incident Response:** Have a plan in place to address production issues, model failures, or performance degradation.")

print("\nDiscussion on model deployment completed.")

## Prediction

### Subtask:
Use the trained model to make predictions on new data.


**Reasoning**:
Use the trained model to make predictions on the test data as instructed.



# Use the .predict() method of the trained model to make predictions on the X_test dataset.
y_pred = model.predict(X_test)

# Store the predictions in a variable named y_pred. (This is already done in the previous line)
y_pred

## Summary:

### Data Analysis Key Findings

*   The dataset contains 1,872,830 transactions with 11 features, including `step`, `type`, `amount`, account balances, and the target variable `isFraud`.
*   There is a severe class imbalance in the target variable `isFraud`, with only 1,882 fraudulent transactions compared to 1,870,947 non-fraudulent ones.
*   A small number of missing values (1) were found in several columns (`nameDest`, `oldbalanceDest`, `newbalanceDest`, `isFraud`, `isFlaggedFraud`).
*   The `isFlaggedFraud` column had only one unique value (0.0) and was not useful for discrimination.
*   The `nameOrig` and `nameDest` columns had very high cardinality.
*   After data preparation (handling missing values, dropping irrelevant columns, one-hot encoding 'type', and scaling numerical features), the data was split into training (1,310,980 rows) and testing (561,849 rows) sets, maintaining the class distribution through stratification.
*   A RandomForestClassifier with balanced class weights was trained to address the class imbalance.
*   The trained model achieved high overall accuracy (1.00).
*   For the fraudulent class (1.0), the model achieved a precision of 0.99 (correctly predicting fraud 99% of the time it predicted fraud) and a recall of 0.73 (identifying 73% of actual fraudulent transactions).
*   The confusion matrix showed 4 false positives (legitimate transactions flagged as fraud) and 151 false negatives (fraudulent transactions missed by the model) on the test set.

### Insights or Next Steps

*   While the model shows high precision for fraud detection, the recall of 0.73 indicates that a significant number of fraudulent transactions are not being identified. Further investigation into optimizing the model or employing advanced data balancing techniques could improve the detection rate of actual fraud.
*   The high cardinality of `nameOrig` and `nameDest` was handled by dropping these columns. Exploring alternative approaches like feature hashing or embedding could potentially incorporate information from these columns into the model if deemed important.


# Get the predicted probabilities for each class
y_pred_proba = model.predict_proba(X_test)

# Display the first few predicted probabilities
print("Predicted probabilities (first 10):")
display(y_pred_proba[:10])

# Task
Create a form to input transaction details and use the trained model to predict the probability of fraud based on the input.

## Create a form

### Subtask:
Generate a code cell with a form to input the transaction details (step, amount, oldbalanceOrg, newbalanceOrig, oldbalanceDest, newbalanceDest, and type).


**Reasoning**:
Create a form with input fields for transaction details and a submit button.



import ipywidgets as widgets
from IPython.display import display

# Create input widgets for numerical features
step_input = widgets.IntText(description='Step:')
amount_input = widgets.FloatText(description='Amount:')
oldbalanceOrg_input = widgets.FloatText(description='Old Balance Originator:')
newbalanceOrig_input = widgets.FloatText(description='New Balance Originator:')
oldbalanceDest_input = widgets.FloatText(description='Old Balance Destination:')
newbalanceDest_input = widgets.FloatText(description='New Balance Destination:')

# Create a dropdown for the 'type' feature
type_options = ['PAYMENT', 'TRANSFER', 'CASH_OUT', 'DEBIT', 'CASH_IN']
type_dropdown = widgets.Dropdown(
    options=type_options,
    description='Type:',
)

# Create a button to submit the form
submit_button = widgets.Button(description='Predict Fraud Probability')

# Arrange the widgets in a vertical box
form = widgets.VBox([
    step_input,
    amount_input,
    oldbalanceOrg_input,
    newbalanceOrig_input,
    oldbalanceDest_input,
    newbalanceDest_input,
    type_dropdown,
    submit_button
])

# Display the form
display(form)

## Process form input

### Subtask:
Capture the form input and convert it into a format suitable for the model.


**Reasoning**:
Define a function to capture the form input, preprocess it to match the training data format, and display the preprocessed data.



def predict_fraud(button):
    """Captures form input, preprocesses it, and makes a fraud probability prediction."""

    # 2. Access the values from the input widgets
    input_data = {
        'step': step_input.value,
        'amount': amount_input.value,
        'oldbalanceOrg': oldbalanceOrg_input.value,
        'newbalanceOrig': newbalanceOrig_input.value,
        'oldbalanceDest': oldbalanceDest_input.value,
        'newbalanceDest': newbalanceDest_input.value,
        'type': type_dropdown.value
    }

    # 3. Create a dictionary and handle one-hot encoding for 'type'
    processed_input = {
        'step': input_data['step'],
        'amount': input_data['amount'],
        'oldbalanceOrg': input_data['oldbalanceOrg'],
        'newbalanceOrig': input_data['newbalanceOrig'],
        'oldbalanceDest': input_data['oldbalanceDest'],
        'newbalanceDest': input_data['newbalanceDest'],
        'type_CASH_OUT': input_data['type'] == 'CASH_OUT',
        'type_DEBIT': input_data['type'] == 'DEBIT',
        'type_PAYMENT': input_data['type'] == 'PAYMENT',
        'type_TRANSFER': input_data['type'] == 'TRANSFER'
    }

    # 4. Convert the dictionary into a pandas DataFrame
    input_df = pd.DataFrame([processed_input])

    # 5. Apply the same scaling transformation to numerical features
    numerical_features = ['amount', 'oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest']

    # Create a new scaler with the *same* parameters as the one used for training data
    # We need to access the original scaler used in the data preparation step (cell_id: d45b3ce7)
    # Assuming the scaler object is named 'scaler' in that cell and is available in the kernel's memory.
    # If not, we would need to re-fit a scaler on the training data (X_train)
    # For this example, we assume 'scaler' is available.

    input_df[numerical_features] = scaler.transform(input_df[numerical_features])


    # 6. Ensure the column order matches X_train
    input_df = input_df[X_train.columns]


    print("\nCaptured and preprocessed input data:")
    display(input_df)

# Link the button click to the function
submit_button.on_click(predict_fraud)

## Make prediction

### Subtask:
Use the trained model to predict the probability of fraud based on the form input.


**Reasoning**:
Use the trained model to predict the probability of fraud based on the preprocessed input data and extract the fraud probability.



def predict_fraud(button):
    """Captures form input, preprocesses it, and makes a fraud probability prediction."""

    # 2. Access the values from the input widgets
    input_data = {
        'step': step_input.value,
        'amount': amount_input.value,
        'oldbalanceOrg': oldbalanceOrg_input.value,
        'newbalanceOrig': newbalanceOrig_input.value,
        'oldbalanceDest': oldbalanceDest_input.value,
        'newbalanceDest': newbalanceDest_input.value,
        'type': type_dropdown.value
    }

    # 3. Create a dictionary and handle one-hot encoding for 'type'
    processed_input = {
        'step': input_data['step'],
        'amount': input_data['amount'],
        'oldbalanceOrg': input_data['oldbalanceOrg'],
        'newbalanceOrig': input_data['newbalanceOrig'],
        'oldbalanceDest': input_data['oldbalanceDest'],
        'newbalanceDest': input_data['newbalanceDest'],
        'type_CASH_OUT': input_data['type'] == 'CASH_OUT',
        'type_DEBIT': input_data['type'] == 'DEBIT',
        'type_PAYMENT': input_data['type'] == 'PAYMENT',
        'type_TRANSFER': input_data['type'] == 'TRANSFER'
    }

    # 4. Convert the dictionary into a pandas DataFrame
    input_df = pd.DataFrame([processed_input])

    # 5. Apply the same scaling transformation to numerical features
    numerical_features = ['amount', 'oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest']

    # Create a new scaler with the *same* parameters as the one used for training data
    # We need to access the original scaler used in the data preparation step (cell_id: d45b3ce7)
    # Assuming the scaler object is named 'scaler' in that cell and is available in the kernel's memory.
    # If not, we would need to re-fit a scaler on the training data (X_train)
    # For this example, we assume 'scaler' is available.

    input_df[numerical_features] = scaler.transform(input_df[numerical_features])


    # 6. Ensure the column order matches X_train
    input_df = input_df[X_train.columns]

    # 1. Use the trained model's .predict_proba() method to get the probability of each class
    y_pred_proba = model.predict_proba(input_df)

    # 2. Extract the probability of fraud (class 1)
    fraud_probability = y_pred_proba[0][1]

    # 3. Print the fraud probability
    print(f"\nPredicted Probability of Fraud: {fraud_probability:.4f}")


# Link the button click to the function
submit_button.on_click(predict_fraud)

## Display result

### Subtask:
Display the predicted probability of fraud.


**Reasoning**:
Display the predicted probability of fraud using the display function.



from IPython.display import display

def predict_fraud(button):
    """Captures form input, preprocesses it, and makes a fraud probability prediction."""

    # 2. Access the values from the input widgets
    input_data = {
        'step': step_input.value,
        'amount': amount_input.value,
        'oldbalanceOrg': oldbalanceOrg_input.value,
        'newbalanceOrig': newbalanceOrig_input.value,
        'oldbalanceDest': oldbalanceDest_input.value,
        'newbalanceDest': newbalanceDest_input.value,
        'type': type_dropdown.value
    }

    # 3. Create a dictionary and handle one-hot encoding for 'type'
    processed_input = {
        'step': input_data['step'],
        'amount': input_data['amount'],
        'oldbalanceOrg': input_data['oldbalanceOrg'],
        'newbalanceOrig': input_data['newbalanceOrig'],
        'oldbalanceDest': input_data['oldbalanceDest'],
        'newbalanceDest': input_data['newbalanceDest'],
        'type_CASH_OUT': input_data['type'] == 'CASH_OUT',
        'type_DEBIT': input_data['type'] == 'DEBIT',
        'type_PAYMENT': input_data['type'] == 'PAYMENT',
        'type_TRANSFER': input_data['type'] == 'TRANSFER'
    }

    # 4. Convert the dictionary into a pandas DataFrame
    input_df = pd.DataFrame([processed_input])

    # 5. Apply the same scaling transformation to numerical features
    numerical_features = ['amount', 'oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest']

    # Create a new scaler with the *same* parameters as the one used for training data
    # We need to access the original scaler used in the data preparation step (cell_id: d45b3ce7)
    # Assuming the scaler object is named 'scaler' in that cell and is available in the kernel's memory.
    # If not, we would need to re-fit a scaler on the training data (X_train)
    # For this example, we assume 'scaler' is available.

    input_df[numerical_features] = scaler.transform(input_df[numerical_features])


    # 6. Ensure the column order matches X_train
    input_df = input_df[X_train.columns]

    # Use the trained model's .predict_proba() method to get the probability of each class
    y_pred_proba = model.predict_proba(input_df)

    # Extract the probability of fraud (class 1)
    fraud_probability = y_pred_proba[0][1]

    # Display the predicted probability of fraud using the display function
    display(f"\nPredicted Probability of Fraud: {fraud_probability:.4f}")


# Link the button click to the function
submit_button.on_click(predict_fraud)

## Summary:

### Data Analysis Key Findings

*   A form was successfully created using `ipywidgets` to capture transaction details, including numerical features and a categorical type.
*   A function `predict_fraud` was developed to process the form input.
*   The processing involved one-hot encoding the categorical 'type' feature and scaling the numerical features using a pre-trained scaler.
*   The preprocessed input was converted into a pandas DataFrame with columns ordered to match the training data used by the model.
*   The trained model's `.predict_proba()` method was used to predict the probability of fraud for the input transaction.
*   The predicted probability of fraud (class 1) was extracted and displayed to the user.

### Insights or Next Steps

*   The interactive form allows for real-time testing of the trained fraud detection model with custom transaction data.
*   Further enhancements could include adding input validation to the form and providing a clearer visual indication of the prediction result (e.g., color-coding based on probability).

